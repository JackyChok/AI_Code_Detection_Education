{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unzip model files and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP-bC408dZQp"
      },
      "outputs": [],
      "source": [
        "!unzip /content/DetectGPT.zip\n",
        "!pip install -r /content/DetectGPT/requirements.txt\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip uninstall -y Pillow\n",
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Empties CUDA memory cache - to avoid CUDA out of memory error\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbc7h9eqj3jm"
      },
      "outputs": [],
      "source": [
        "cd DetectGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive to save outputs to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsGdFCpP1M_k"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNnTFCK63l_p"
      },
      "outputs": [],
      "source": [
        "# Run DetectGPT with user input\n",
        "# !python local_infer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AIGC Detector execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZGiDLgMvXcU"
      },
      "outputs": [],
      "source": [
        "from model import GPT2PPLV2 as GPT2PPL\n",
        "import csv\n",
        "import sys\n",
        "import torch\n",
        "import shutil\n",
        "\n",
        "# input_file = '/content/DetectGPT/data/variant_1_full.csv'     # Replace input file path to file containing variant content\n",
        "# output_file = '/content/DetectGPT/data/output_variant_1.csv'  # Replace output file path to file containing DetectGPT outputs\n",
        "\n",
        "# Initialize the model\n",
        "model = GPT2PPL()\n",
        "\n",
        "# Open input and output files\n",
        "with open(input_file, newline='', encoding=\"utf-8\") as infile, open(output_file, 'w', newline='', encoding=\"utf-8\") as outfile:\n",
        "\n",
        "    # Create reader and writer objects\n",
        "    reader_obj = csv.reader(infile)\n",
        "    writer_obj = csv.writer(outfile)\n",
        "\n",
        "    # Read and write the header, and add a new column for results\n",
        "    header = next(reader_obj)\n",
        "    header.append(\"DetectGPT_answer_human_full\")\n",
        "    header.append(\"DetectGPT_answer_GPT_full\")\n",
        "    writer_obj.writerow(header)\n",
        "\n",
        "    # Iterate over each row in the csv file using reader object\n",
        "    for idx, row in enumerate(reader_obj):\n",
        "        # Get the python code and GPT code in current row\n",
        "        python_code = row[4]\n",
        "        gpt_answer = row[5]\n",
        "\n",
        "        # Run the model on the sentence, with 100 tokens using version 1.1\n",
        "        try:\n",
        "          detectGPT_answer_human_full = model(python_code, 100, \"v1.1\")\n",
        "          print('Probability for Human: {}'.format(detectGPT_answer_human_full))\n",
        "          row.append(detectGPT_answer_human_full)\n",
        "        except:\n",
        "          pass\n",
        "        torch.cuda.empty_cache()\n",
        "        try:\n",
        "          detectGPT_answer_GPT_full = model(gpt_answer, 100, \"v1.1\")\n",
        "          print('Probability for GPT: {}'.format(detectGPT_answer_GPT_full))\n",
        "          row.append(detectGPT_answer_GPT_full)\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        # Append the result to the row and write it to the output file\n",
        "        writer_obj.writerow(row)\n",
        "        print('Done -- index {}!\\n'.format(row[0]))\n",
        "\n",
        "        # Replace paths to shutil.copy(path to DetectGPT output file, path to save DetectGPT outputs in mounted GDrive)\n",
        "        # shutil.copy(\"/content/DetectGPT/data/output_variant_1.csv\", \"/content/drive/My Drive/VariantOutputs/\") \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
